Here is the updated specification, v4.2, integrating all of your requested changes.

***

## Final Software Specification (v4.2)

This document outlines a content analysis and NLP platform, moving from data ingestion and transformation (ETL) to automated analysis and user-facing interaction.

### 1. Technology Stack Summary

* **Backend (Core):** **FastAPI**
* **Database:** **SQLite** (development), **PostgreSQL** (production)
* **Database ORM:** **SQLAlchemy**
* **NLP (Core):** **spaCy**
* **NLP (Models):** spaCy models for **English** (e.g., `en_core_web_sm`) and **Spanish** (e.g., `es_core_news_sm`)
* **Clustering/ML:** **scikit-learn**
* **OCR:** **Tesseract**
* **Backend Task Queue:** **Celery & Redis**

### 2. User Personas

* **`Admin`:** Configures and monitors the ingestion pipeline; schedules and monitors the NLP analysis jobs.
* **`Data Analyst`:** Logs in to search, view, and annotate content; consumes the *results* of the automated NLP jobs.

### 3. Functional Requirements (User Stories)

#### Module 1: The Data Pipeline (Ingestion & Storage)

* **Story 1 (Extract):** As an **`Admin`**, I want to configure sources, which can be websites, RSS feeds, or local directories containing files (`.txt`, `.md`, `.csv`, `.docx`, `.xlsx`, `.pdf`, `.jpg`, `.png`, `.webp`).
    * **1a (OCR):** As an **`Admin`**, when the pipeline detects an image (`.jpg`, `.png`, `.webp`), I want the system to automatically run **Tesseract** to extract text.
    * **1b (Parsers):** As an **`Admin`**, I want the system to use the correct parser for each file type (`.pdf`, `.docx`, etc.).
    * **1c (Excel Handling):** As an **`Admin`**, when the pipeline detects an `.xlsx` file, I want the system to first convert it to an in-memory `.csv` representation.
* **Story 2 (Schedule):** As an **`Admin`**, I want to be able to schedule ingestion jobs to run at specific intervals.
* **Story 3 (Transform):** As an **`Admin`**, I want the system to sanitize and transform all extracted content into a standardized "Article" data model.
    * **3a (Structured Data):** **(Updated)** For `.csv` and converted `.xlsx` files, each **row** must be treated as a separate document and stored in the `content_text` field of a **new, distinct `Article` entity**.
* **Story 4 (Load):** As an **`Admin`**, I want the transformed "Article" data to be loaded and indexed (for full-text search) into the **PostgreSQL database**.
* **Story 5 (Audit):** As an **`Admin`**, I want to view a detailed pipeline activity log, filterable by date, source, and status (Success/Failure).

---

#### Module 2: The Content Application (Interaction)

* **Story 6 (Auth):** As a user, I want to log in with a username and password. The system must **only grant access if my account is marked as `is_active`**.
* **Story 7 (Search):** As a **`Data Analyst`**, I want to perform a full-text search (using PostgreSQL's built-in FTS) across the content of all articles.
* **Story 8 (Filter):** As a **`Data Analyst`**, I want to filter my search results by metadata (source, date range, tags).
* **Story 9 (View):** As a **`Data Analyst`**, I want to open a search result to view the clean, transformed article content.
* **Story 10 (Annotate - Tag):** As a **`Data Analyst`**, I want to be able to add one or more **tags** to any article.
* **Story 11 (Annotate - Comment):** As a **`Data Analyst`**, I want to be able to add free-text **comments** or notes to an article.
* **Story 12 (Export):** As a **`Data Analyst`**, I want to be able to select one or more articles and **export** them (e.g., as CSV or JSON).

---

#### Module 3: The Data Analysis (Automated Processing)

* **Story 13 (Job Scheduling):** As an **`Admin`**, I want to schedule all major NLP jobs (NER, Clustering, Association Rules) to run automatically at a defined interval (e.g., nightly).
* **Story 14 (Job Scope):** As the **System**, when a scheduled NLP job runs, it must **only process articles** that have a `last_nlp_run_timestamp` older than the start time of the *previous* job, ensuring no documents are skipped and work is not duplicated.
* **Story 15 (Job Status):** As a **`Data Analyst`**, I want to see a "Last Analysis Run" timestamp in the UI so I know how fresh the NLP data (entities, clusters) is.
* **Story 16 (View Word Cloud):** **(Updated)** As a **`Data Analyst`**, I want to select one or more articles and request a **word cloud**. The system must generate this asynchronously (using the **Celery** queue) and notify the UI when the visualization is ready.
* **Story 17 (View NER):** As a **`Data Analyst`**, I want to view the list of named entities (People, Orgs, etc.) that were **automatically extracted** by the last scheduled **spaCy** job for any given article.
* **Story 18 (View Clusters):** As a **`Data Analyst`**, I want to be able to view the topic or entity clusters (generated by **scikit-learn**) and see all articles that belong to a specific cluster.
* **Story 19 (View Rules):** As a **`Data Analyst`**, I want to browse the **association rules** generated by the last scheduled job (e.g., "See terms most associated with 'Project X'").

---

#### Module 4: Administration & Management (CLI)

* **Story 20 (Database Init):** As an **`Admin`**, I want to run a single command... to create the database...
* **Story 21 (Add User):** As an **`Admin`**, I want a CLI command to create a new user by providing a username, a password, and a role (`admin` or `data_analyst`).
* **Story 22 (Toggle User):** As an **`Admin`**, I want a CLI command to **deactivate (block)** or **reactivate (unblock)** a user by their username. This command will toggle their `is_active` status.
* **Story 23 (Add Source):** As an **`Admin`**, I want a CLI command to add a new data source...
* **Story 24 (Toggle Source):** As an **`Admin`**, I want a CLI command to block (deactivate) or unblock (reactivate) a specific data source.

### 4. Relational Data Model (PostgreSQL)

* `User`:
    * `user_id` (PK)
    * `username` (Unique)
    * `hashed_password`
    * `role` ('admin', 'data_analyst')
    * `is_active` (Boolean, default: true)
* `Source`:
    * `source_id` (PK)
    * `name`, `type`, `location`, `last_run_timestamp`
* `Article`:
    * `article_id` (PK)
    * `source_id` (FK to `Source`, ON DELETE SET NULL)
    * `title`, `content_text`, `original_url`
    * `source_format` (e.g., 'pdf', 'rss', 'csv-row', 'png')
    * `extraction_date`
    * `content_text_vector` (A `tsvector` column for FTS)
    * `last_nlp_run_timestamp` (Timestamp, nullable. Used by Story 14)
* `Annotation`:
    * `annotation_id` (PK)
    * `article_id` (FK to `Article`, **ON DELETE CASCADE**)
    * `user_id` (FK to `User`, ON DELETE SET NULL)
    * `type` ('TAG', 'COMMENT'), `content`, `created_at`
* `NamedEntity`:
    * `entity_id` (PK)
    * `entity_text` (e.g., "Google"), `entity_type` (e.g., 'ORG'), `language` ('en' or 'es')
* `ArticleEntity`: (Join table)
    * `article_id` (FK to `Article`, **ON DELETE CASCADE**)
    * `entity_id` (FK to `NamedEntity`, **ON DELETE CASCADE**)
* `Cluster`:
    * `cluster_id` (PK)
    * `cluster_name` (e.g., "Topic 1: Finance"), `cluster_type` ('TOPIC', 'ENTITY'), `last_run_id`
* `ArticleCluster`: (Join table)
    * `article_id` (FK to `Article`, **ON DELETE CASCADE**)
    * `cluster_id` (FK to `Cluster`, **ON DELETE CASCADE**)
* `PipelineLog`: (Unchanged)

### 5. Non-Functional Requirements (NFRs)

1.  **Database:** Must use **SQLite** for development and **PostgreSQL** for production.
2.  **Search:** Full-text search must use **PostgreSQL's native FTS** (`tsvector`/`tsquery`).
3.  **Asynchronous Processing:** All Module 3 NLP jobs **must** run as asynchronous background tasks on a schedule, separate from the web application.
4.  **Multi-language:** The NLP pipeline must load and use **spaCy** models for both **English** and **Spanish** to process documents.
5.  **OCR:** The system must use **Tesseract** for all image-to-text extraction.
6.  **Fault-Tolerance:** The ingestion pipeline must log errors from a single failed source (e.g., 404 URL) and continue processing the rest of the sources.
7.  **(New) Configuration:** All application settings, including database connections, source locations, and queue credentials, must be managed externally via **YAML configuration files**.
8.  **(New) Testing (Coverage):** The system must have >80% unit test coverage.
9.  **(New) Testing (Integration):** All pipeline parsers must have integration tests.
10. **(New) Testing (UI):** UI tests must cover the login and search workflows.
11. **(New) Security (Auth):** All web application endpoints must be secured, requiring an authenticated session.
12. **(New) Security (Transport):** The system must use SSL/TLS in production.
13. **(New) Security (CLI):** Admin CLI scripts must require authenticated credentials.
